#  Gemini Chat GUI with Local Emotion Detection
# Interact with the chatbot below.

import os
import joblib
import numpy as np
import streamlit as st
from dotenv import load_dotenv
import google.generativeai as genai

def preprocess(text, nlp):
    text = text.lower()
    text = ''.join([c for c in text if c.isalnum() or c.isspace()])
    doc = nlp(text)
    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and token.lemma_ != '-PRON-' and token.lemma_.strip() != '']
    return ' '.join(tokens)

# Load API key from environment (safe for GitHub use)
load_dotenv()
api_key = os.getenv('GEMINI_API_KEY')
genai.configure(api_key=api_key)

# Load the model and vectorizer with explicit numpy array handling
try:
    model = joblib.load('model.pkl', mmap_mode=None)
    vectorizer = joblib.load('vectorizer.pkl', mmap_mode=None)
except Exception as e:
    print(f"Error loading model: {str(e)}")
    raise

# Load spaCy model
import spacy
nlp = spacy.load('en_core_web_sm')

# Streamlit UI
st.set_page_config(layout="centered")
st.title(' Mood Miner Chat â€“ PIXIE the Emotion-Aware Assistant')

# Custom CSS for chat styling
st.markdown("""
    <style>
        .stApp {
            display: flex;
            flex-direction: column;
            height: 100vh;
        }
        .chat-container {
            flex-grow: 1;
            overflow-y: auto;
            display: flex;
            flex-direction: column-reverse;
            padding: 10px;
        }
        .chat-message {
            padding: 10px;
            border-radius: 10px;
            margin: 5px;
            display: inline-block;
            max-width: 80%;
            word-wrap: break-word;
        }
        .user-message-container {
            display: flex;
            justify-content: flex-end;
        }
        .bot-message-container {
            display: flex;
            justify-content: flex-start;
        }
        .user-message {
            background-color: #DCF8C6;
            color: #000;
        }
        .bot-message {
            background-color: #EAEAEA;
            color: #000;
        }
    </style>
""", unsafe_allow_html=True)

if 'chat_history' not in st.session_state:
    st.session_state['chat_history'] = []

# Chat history display
chat_container = st.container()
with chat_container:
    st.markdown('<div class="chat-container">', unsafe_allow_html=True)
    for msg in st.session_state['chat_history']:
        st.markdown(f'<div class="user-message-container"><div class="chat-message user-message"><b>You:</b> {msg["user"]}</div></div>', unsafe_allow_html=True)
        st.markdown(f'<div class="bot-message-container"><div class="chat-message bot-message"><b>Pixie (Emotion: *{msg["emotion"]}*):</b> {msg["bot"]}</div></div>', unsafe_allow_html=True)
    st.markdown('</div>', unsafe_allow_html=True)

if user_input := st.chat_input("Say something"):
    cleaned = preprocess(user_input, nlp)
    vect = vectorizer.transform([cleaned])
    probs = model.predict_proba(vect)[0]
    max_prob = np.max(probs)
    emotion = model.classes_[np.argmax(probs)]
    threshold = 0.5

    if max_prob < threshold:
        emotion = 'neutral'
        prompt = (
            f"You are Pixie, a friendly and intelligent assistant. The user said: '{user_input}'. "
            f"Your goal is to have a normal, helpful conversation. "
            f"Respond to the user's query directly and naturally. "
            f"Maintain a friendly and engaging tone."
        )
    else:
        prompt = (
            f"You are Pixie, an empathetic and intelligent assistant. The user is feeling '{emotion}' and said: '{user_input}'. "
            f"Your goal is to provide a supportive and insightful response. "
            f"First, respond directly to the user's query. "
            f"Then, at the end of your response, provide a relevant suggestion, quote, song, or activity to help them based on their emotion. "
            f"All suggestions should be generated by you, the LLM. "
            f"Avoid mentioning the detected emotion directly unless it feels natural and contextually appropriate. "
            f"Maintain a friendly and caring tone."
        )

    if st.session_state['chat_history']:
        history = '\n'.join([f"User: {msg['user']}\nBot: {msg['bot']}" for msg in st.session_state['chat_history']])
        prompt = f"Previous conversation:\n{history}\n\n" + prompt
    
    chat = genai.GenerativeModel('gemini-1.5-flash').start_chat()
    gemini_response = chat.send_message(prompt).text
    st.session_state['chat_history'].append({'user': user_input, 'bot': gemini_response, 'emotion': emotion})
    st.rerun()
